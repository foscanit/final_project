{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex text preprocessing tasks\n",
    "\n",
    "If I had a better computer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16., 9.)})\n",
    "sns.set_style('whitegrid')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocessing_chunked_save(five, chunk_size=100, folder_path=None):\n",
    "    # Create an empty list to store the preprocessed data\n",
    "    preprocessed_data_list = []\n",
    "\n",
    "    for chunk_start in range(0, len(five), chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        chunk = five.iloc[chunk_start:chunk_end]\n",
    "\n",
    "        for index, row in chunk.iterrows():\n",
    "            text = row['summary']\n",
    "\n",
    "            # Lowercasing\n",
    "            text = text.lower()\n",
    "\n",
    "            # Tokenization\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            # Removing stop words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "            # Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "            # Removing special characters and numbers\n",
    "            clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "\n",
    "            # Append the preprocessed text to the list\n",
    "            preprocessed_data_list.append({'cleaned_summary': ' '.join(clean_tokens)})\n",
    "\n",
    "        # Save the preprocessed data to a CSV file if specified\n",
    "        if folder_path:\n",
    "            output_file = os.path.join(folder_path, f'preprocessed_data_chunk_{chunk_start}_{chunk_end}.csv')\n",
    "            pd.DataFrame(preprocessed_data_list).to_csv(output_file, index=False)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame after the loop\n",
    "    preprocessed_data = pd.DataFrame(preprocessed_data_list)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Example usage:\n",
    "# preprocessed_data = preprocessing_chunked_save(your_dataframe, chunk_size=100, folder_path='/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessing_chunked_save(five, chunk_size=100, folder_path='/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Set the folder path where the CSV files are located\n",
    "folder_path = '/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data'\n",
    "\n",
    "# Find all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, 'preprocessed_data_chunk_*.csv'))\n",
    "\n",
    "# Create an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame(columns=['cleaned_summary'])\n",
    "\n",
    "# Read each CSV file and append its content to the combined DataFrame\n",
    "for csv_file in csv_files:\n",
    "    chunk_data = pd.read_csv(csv_file)\n",
    "    combined_data = pd.concat([combined_data, chunk_data], ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_data.to_csv(os.path.join(folder_path, 'preprocessed_combined_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = pd.read_csv(\"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data/preprocessed_combined_data.csv\")\n",
    "lemmas.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: text preprocessing without punctuation removal\n",
    "# To optimize the following function (make the code more efficient) I should try to use this library:\n",
    "# pycontractions 2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocessing_0(five):\n",
    "    spell_checker = SpellChecker()\n",
    "    \n",
    "    for index, row in five.iterrows():\n",
    "        text = row['summary']\n",
    "        \n",
    "        # Handling Contractions with Context\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)           # e.g., don't -> do not, isn't -> is not\n",
    "        text = re.sub(r\"'re\", \" are\", text)           # e.g., you're -> you are\n",
    "        text = re.sub(r\"'s\", \" is\", text)             # e.g., he's -> he is\n",
    "        text = re.sub(r\"'d\\b\", \" would\", text)        # e.g., I'd -> I would (if followed by a word boundary)\n",
    "        text = re.sub(r\"'d(?=\\s|$)\", \" had\", text)    # e.g., he'd -> he had (if followed by whitespace or end of string)\n",
    "        text = re.sub(r\"'ll\", \" will\", text)          # e.g., you'll -> you will\n",
    "        text = re.sub(r\"'ve\", \" have\", text)          # e.g., they've -> they have\n",
    "\n",
    "        # Additional Cases for Distinguishing 'had' and 'would'\n",
    "        text = re.sub(r\"he'd\\b\", \"he would\", text)    # e.g., he'd -> he would (if followed by a word boundary)\n",
    "        text = re.sub(r\"he'd(?=\\s|$)\", \"he had\", text)# e.g., he'd -> he had (if followed by whitespace or end of string)\n",
    "\n",
    "        text = re.sub(r\"we'd\\b\", \"we would\", text)    # e.g., we'd -> we would\n",
    "        text = re.sub(r\"we'd(?=\\s|$)\", \"we had\", text)# e.g., we'd -> we had\n",
    "\n",
    "        text = re.sub(r\"they'd\\b\", \"they would\", text)    # e.g., they'd -> they would\n",
    "        text = re.sub(r\"they'd(?=\\s|$)\", \"they had\", text)# e.g., they'd -> they had\n",
    "\n",
    "        text = re.sub(r\"you'd\\b\", \"you would\", text)    # e.g., you'd -> you would\n",
    "        text = re.sub(r\"you'd(?=\\s|$)\", \"you had\", text)# e.g., you'd -> you had\n",
    "\n",
    "        text = re.sub(r\"I'd\\b\", \"I would\", text)    # e.g., I'd -> I would\n",
    "        text = re.sub(r\"I'd(?=\\s|$)\", \"I had\", text)# e.g., I'd -> I had\n",
    "\n",
    "        text = re.sub(r\"that'd\\b\", \"that would\", text)    # e.g., that'd -> that would\n",
    "        text = re.sub(r\"that'd(?=\\s|$)\", \"that had\", text)# e.g., that'd -> that had\n",
    "\n",
    "        text = re.sub(r\"she'd\\b\", \"she would\", text)      # e.g., she'd -> she would (if followed by a word boundary)\n",
    "        text = re.sub(r\"she'd(?=\\s|$)\", \"she had\", text)  # e.g., she'd -> she had (if followed by whitespace or end of string)\n",
    "\n",
    "\n",
    "        # Spell Correction\n",
    "        tokens = word_tokenize(text)\n",
    "        corrected_tokens = [spell_checker.correction(word) for word in tokens]\n",
    "        filtered_corrected_tokens = [word for word in corrected_tokens if word is not None]\n",
    "        text = ' '.join(filtered_corrected_tokens)\n",
    "\n",
    "        # Sentence Segmentation\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Assuming you want to concatenate sentences with a space in between\n",
    "        text = ' '.join(sentences)\n",
    "        \n",
    "        # Lowercasing if it's not an abbreviation.\n",
    "        if re.match('([A-Z]+[a-z]*){2,}', text):\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.lower() \n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Removing special characters and numbers\n",
    "        clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "        \n",
    "        # Update the 'cleaned_summary' column with the preprocessed text\n",
    "        five.at[index, 'cleaned_summary'] = ' '.join(clean_tokens)\n",
    "        five['cleaned_summary'] = five['cleaned_summary'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: text preprocessing with punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: text preprocessing with stemming instead of lemmatization\n",
    "# SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4: text preprocessing with handling rare words \n",
    "# (remove or replace rare words that might not contribute much to the model's understanding). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Analysis is a process in machine learning where you assess the significance or contribution of each feature (in this case, words) to the model's predictions. It helps you understand which features have the most impact on the model's performance. In the context of text data, features are often individual words or tokens.\n",
    "\n",
    "Here's how you might conduct a Feature Importance Analysis for your text classification task:\n",
    "\n",
    "Train a Model:\n",
    "\n",
    "Train your machine learning model using your preprocessed text data.\n",
    "Use a Model with Inherent Feature Importance:\n",
    "\n",
    "Some models, like decision trees or random forests, have built-in mechanisms for calculating feature importance during training. These models can provide a direct measure of how much each word contributes to the model's decisions.\n",
    "Feature Importance Metrics:\n",
    "\n",
    "For models without built-in feature importance, you can use techniques like permutation importance or SHAP (SHapley Additive exPlanations) values. These methods provide insights into how much the inclusion or exclusion of a feature affects the model's predictions.\n",
    "Visualization:\n",
    "\n",
    "Visualize the feature importance scores. This could be in the form of a bar chart, where each bar represents the importance of a specific word.\n",
    "Identify Influential Words:\n",
    "\n",
    "Analyze the feature importance results to identify which words are the most influential in making predictions. This includes understanding whether rare words, in particular, play a significant role.\n",
    "Why Feature Importance Analysis Matters:\n",
    "Identifying Key Features:\n",
    "\n",
    "It helps you identify which words or features are crucial for the model's decision-making process. This insight is valuable for understanding the interpretability of your model.\n",
    "Optimizing Preprocessing:\n",
    "\n",
    "You can use the results to optimize your preprocessing steps. If rare words turn out to be important, you might reconsider strategies for handling them.\n",
    "Model Understanding:\n",
    "\n",
    "Feature importance analysis provides a way to interpret your model's behavior. It helps answer questions like: \"What words contribute the most to predicting a certain genre?\"\n",
    "Addressing Overfitting:\n",
    "\n",
    "It can help identify if the model is overfitting to specific words, potentially leading to more robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_train is your feature matrix and y_train is the target variable\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Get the words (features)\n",
    "words = your_feature_names  # Replace with your actual feature names (words)\n",
    "\n",
    "# Create a bar chart for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
