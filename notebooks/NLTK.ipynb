{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d019b4",
   "metadata": {},
   "source": [
    "### 6. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use NLTK for the preprocessing of the summaries:\n",
    "\n",
    "- Lowercasing: Convert all text to lowercase to maintain consistency.\n",
    "- Tokenization: Split the text into individual words (tokens).\n",
    "- Removing stop words.\n",
    "- Lemmatization or stemming: reduce words to their base or root form to normalize variations.\n",
    "- Removing special characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63783401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16., 9.)})\n",
    "sns.set_style('whitegrid')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2f4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7388683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>science fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0              A Clockwork Orange   \n",
       "1                      The Plague   \n",
       "2  All Quiet on the Western Front   \n",
       "\n",
       "                                             summary             genre  \n",
       "0   Alex, a teenager living in near-future Englan...   science fiction  \n",
       "1   The text of The Plague is divided into five p...  literary fiction  \n",
       "2   The book tells the story of Paul Bäumer, a Ge...  literary fiction  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five = pd.read_csv(\"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data/five.csv\")\n",
    "five.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129f58b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11013, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: text preprocessing without punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker  \n",
    "\n",
    "def preprocessing_1(five):\n",
    "    spell_checker = SpellChecker()\n",
    "    \n",
    "    for index, row in five.iterrows():\n",
    "        text = row['summary']\n",
    "        \n",
    "        # Handling Contractions\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        # Add more contraction expansions as needed\n",
    "        \n",
    "        # Spell Correction\n",
    "        tokens = word_tokenize(text)\n",
    "        corrected_tokens = [spell_checker.correction(word) for word in tokens]\n",
    "        text = ' '.join(corrected_tokens)\n",
    "        \n",
    "        # Sentence Segmentation\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Assuming you want to concatenate sentences with a space in between\n",
    "        text = ' '.join(sentences)\n",
    "        \n",
    "        # Lowercasing if it's not an abbreviation.\n",
    "        if re.match('([A-Z]+[a-z]*){2,}', text):\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.lower() \n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Removing special characters and numbers\n",
    "        clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "        \n",
    "        # Update the 'cleaned_summary' column with the preprocessed text\n",
    "        five.at[index, 'cleaned_summary'] = ' '.join(clean_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Handling Contractions:\n",
    "Expand contractions to ensure consistency. For example, convert \"don't\" to \"do not.\"\n",
    "\n",
    "2) Spell Correction:\n",
    "Depending on the quality of your data, you might consider implementing a spell-checking mechanism to correct typos.\n",
    "\n",
    "3) Sentence Segmentation (before tokenization):\n",
    "If your summaries are long and contain multiple sentences, consider segmenting them into individual sentences.\n",
    "\n",
    "\n",
    "4) OUTSIDE THE FUNCTION AND IN ANOTHER COLUMN - Part-of-Speech Tagging:\n",
    "\n",
    "a - Perform part-of-speech tagging to understand the grammatical structure of sentences. This can be useful for certain types of analysis.\n",
    "b - NER: name entity recognition\n",
    "\n",
    "5) Add a column with the len of each summary, so that then I can do a groupby and plot a histogram for each genre. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(five):\n",
    "    for index, row in five.iterrows():\n",
    "        text = row['summary']\n",
    "        \n",
    "        # Lowercasing if it's not an abbreviation.\n",
    "        if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.lower() \n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Removing special characters and numbers\n",
    "        clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "        \n",
    "        # Update the 'cleaned_summary' column with the preprocessed text\n",
    "        five.at[index, 'cleaned_summary'] = ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: text preprocessing with punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: text preprocessing with stemming instead of lemmatization\n",
    "# SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4: text preprocessing with handling rare words \n",
    "# (remove or replace rare words that might not contribute much to the model's understanding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to find out the summary count of words (the average) for each genre.\n",
    "# I also want to find out the summary count of unique words (average) for each genre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Analysis is a process in machine learning where you assess the significance or contribution of each feature (in this case, words) to the model's predictions. It helps you understand which features have the most impact on the model's performance. In the context of text data, features are often individual words or tokens.\n",
    "\n",
    "Here's how you might conduct a Feature Importance Analysis for your text classification task:\n",
    "\n",
    "Train a Model:\n",
    "\n",
    "Train your machine learning model using your preprocessed text data.\n",
    "Use a Model with Inherent Feature Importance:\n",
    "\n",
    "Some models, like decision trees or random forests, have built-in mechanisms for calculating feature importance during training. These models can provide a direct measure of how much each word contributes to the model's decisions.\n",
    "Feature Importance Metrics:\n",
    "\n",
    "For models without built-in feature importance, you can use techniques like permutation importance or SHAP (SHapley Additive exPlanations) values. These methods provide insights into how much the inclusion or exclusion of a feature affects the model's predictions.\n",
    "Visualization:\n",
    "\n",
    "Visualize the feature importance scores. This could be in the form of a bar chart, where each bar represents the importance of a specific word.\n",
    "Identify Influential Words:\n",
    "\n",
    "Analyze the feature importance results to identify which words are the most influential in making predictions. This includes understanding whether rare words, in particular, play a significant role.\n",
    "Why Feature Importance Analysis Matters:\n",
    "Identifying Key Features:\n",
    "\n",
    "It helps you identify which words or features are crucial for the model's decision-making process. This insight is valuable for understanding the interpretability of your model.\n",
    "Optimizing Preprocessing:\n",
    "\n",
    "You can use the results to optimize your preprocessing steps. If rare words turn out to be important, you might reconsider strategies for handling them.\n",
    "Model Understanding:\n",
    "\n",
    "Feature importance analysis provides a way to interpret your model's behavior. It helps answer questions like: \"What words contribute the most to predicting a certain genre?\"\n",
    "Addressing Overfitting:\n",
    "\n",
    "It can help identify if the model is overfitting to specific words, potentially leading to more robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_train is your feature matrix and y_train is the target variable\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Get the words (features)\n",
    "words = your_feature_names  # Replace with your actual feature names (words)\n",
    "\n",
    "# Create a bar chart for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(words, feature_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
