{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d019b4",
   "metadata": {},
   "source": [
    "### 6. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use NLTK for the preprocessing of the summaries:\n",
    "\n",
    "- Lowercasing: Convert all text to lowercase to maintain consistency.\n",
    "- Tokenization: Split the text into individual words (tokens).\n",
    "- Removing stop words.\n",
    "- Lemmatization or stemming: reduce words to their base or root form to normalize variations.\n",
    "- Removing special characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63783401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "%config Inlinebackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set(rc={'figure.figsize': (16., 9.)})\n",
    "sns.set_style('whitegrid')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2f4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7388683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>science fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul B채umer, a Ge...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0              A Clockwork Orange   \n",
       "1                      The Plague   \n",
       "2  All Quiet on the Western Front   \n",
       "\n",
       "                                             summary             genre  \n",
       "0   Alex, a teenager living in near-future Englan...   science fiction  \n",
       "1   The text of The Plague is divided into five p...  literary fiction  \n",
       "2   The book tells the story of Paul B채umer, a Ge...  literary fiction  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five = pd.read_csv(\"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data/five.csv\")\n",
    "five.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129f58b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11013, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocessing_1(five):\n",
    "    \n",
    "    for index, row in five.iterrows():\n",
    "        text = row['summary']\n",
    "        \n",
    "        # Lowercasing\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Removing special characters and numbers\n",
    "        clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "        \n",
    "        # Update the 'cleaned_summary' column with the preprocessed text\n",
    "        five.at[index, 'cleaned_summary'] = ' '.join(clean_tokens)\n",
    "        five['cleaned_summary'] = five['cleaned_summary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_1(five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>alex  teenager living nearfuture england  lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>text plague divided five part  town oran  thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul B채umer, a Ge...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>book tell story paul bumer  german soldier who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Wizard of Earthsea</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>ged young boy gont  one larger island north ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blade Runner 3: Replicant Night</td>\n",
       "      <td>Living on Mars, Deckard is acting as a consul...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>living mar  deckard acting consultant movie cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0               A Clockwork Orange   \n",
       "1                       The Plague   \n",
       "2   All Quiet on the Western Front   \n",
       "3             A Wizard of Earthsea   \n",
       "4  Blade Runner 3: Replicant Night   \n",
       "\n",
       "                                             summary             genre  \\\n",
       "0   Alex, a teenager living in near-future Englan...   science fiction   \n",
       "1   The text of The Plague is divided into five p...  literary fiction   \n",
       "2   The book tells the story of Paul B채umer, a Ge...  literary fiction   \n",
       "3   Ged is a young boy on Gont, one of the larger...           fantasy   \n",
       "4   Living on Mars, Deckard is acting as a consul...   science fiction   \n",
       "\n",
       "                                     cleaned_summary  \n",
       "0  alex  teenager living nearfuture england  lead...  \n",
       "1  text plague divided five part  town oran  thou...  \n",
       "2  book tell story paul bumer  german soldier who...  \n",
       "3  ged young boy gont  one larger island north ar...  \n",
       "4  living mar  deckard acting consultant movie cr...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLUS:\n",
    "\n",
    "1) NER: name entity recognition\n",
    "\n",
    "2) Add a column with the len of each summary, so that then I can do a groupby and plot a histogram for each genre. \n",
    "\n",
    "3) Add a column with the len of unique words of each summary, so that I can do a value_counts of which genre has more unique words \n",
    "(probably the fantasy genre). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_2(five):\n",
    "    for index, row in five.iterrows():\n",
    "        text = row['summary']\n",
    "        \n",
    "        # Lowercasing if it's not an abbreviation.\n",
    "        if re.match('([A-Z]+[a-z]*){2,}', text):\n",
    "            text = text\n",
    "        else:\n",
    "            text = text.lower() \n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        \n",
    "        # Removing special characters and numbers\n",
    "        clean_tokens = [re.sub(r'[^a-zA-Z]', '', word) for word in lemmatized_tokens]\n",
    "        \n",
    "        # Update the 'cleaned_summary' column with the preprocessed text\n",
    "        five.at[index, 'cleaned_summary'] = ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
