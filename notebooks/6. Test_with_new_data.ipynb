{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre</th>\n",
       "      <th>cleaned_summary</th>\n",
       "      <th>entities</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>alex teenager living near future england lead ...</td>\n",
       "      <td>[('alex', 'PERSON'), ('england', 'GPE'), ('rus...</td>\n",
       "      <td>588</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>text plague divided five part town oran thousa...</td>\n",
       "      <td>[('five', 'CARDINAL'), ('dr bernard rieux', 'P...</td>\n",
       "      <td>609</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>book tell story paul umer german soldier who u...</td>\n",
       "      <td>[('paul umer', 'PERSON'), ('german', 'NORP'), ...</td>\n",
       "      <td>375</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Wizard of Earthsea</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>ged young boy gont one larger island north arc...</td>\n",
       "      <td>[('gont', 'PERSON'), ('one', 'CARDINAL'), ('ar...</td>\n",
       "      <td>549</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0              A Clockwork Orange   \n",
       "1                      The Plague   \n",
       "2  All Quiet on the Western Front   \n",
       "3            A Wizard of Earthsea   \n",
       "\n",
       "                                             summary             genre  \\\n",
       "0   Alex, a teenager living in near-future Englan...   science fiction   \n",
       "1   The text of The Plague is divided into five p...  literary fiction   \n",
       "2   The book tells the story of Paul Bäumer, a Ge...  literary fiction   \n",
       "3   Ged is a young boy on Gont, one of the larger...           fantasy   \n",
       "\n",
       "                                     cleaned_summary  \\\n",
       "0  alex teenager living near future england lead ...   \n",
       "1  text plague divided five part town oran thousa...   \n",
       "2  book tell story paul umer german soldier who u...   \n",
       "3  ged young boy gont one larger island north arc...   \n",
       "\n",
       "                                            entities  word_count  \\\n",
       "0  [('alex', 'PERSON'), ('england', 'GPE'), ('rus...         588   \n",
       "1  [('five', 'CARDINAL'), ('dr bernard rieux', 'P...         609   \n",
       "2  [('paul umer', 'PERSON'), ('german', 'NORP'), ...         375   \n",
       "3  [('gont', 'PERSON'), ('one', 'CARDINAL'), ('ar...         549   \n",
       "\n",
       "   unique_word_count  \n",
       "0                416  \n",
       "1                424  \n",
       "2                277  \n",
       "3                371  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/final_project/data/model_data.csv\")\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_summary</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>alex teenager living near future england lead ...</td>\n",
       "      <td>science fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>text plague divided five part town oran thousa...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>book tell story paul umer german soldier who u...</td>\n",
       "      <td>literary fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0              A Clockwork Orange   \n",
       "1                      The Plague   \n",
       "2  All Quiet on the Western Front   \n",
       "\n",
       "                                     cleaned_summary             genre  \n",
       "0  alex teenager living near future england lead ...   science fiction  \n",
       "1  text plague divided five part town oran thousa...  literary fiction  \n",
       "2  book tell story paul umer german soldier who u...  literary fiction  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = ['title', 'cleaned_summary', \"genre\"]\n",
    "check = data[selected_columns].copy()\n",
    "check.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Glasshouse',\n",
       " 'Exiles to Glory',\n",
       " 'Nova Express',\n",
       " 'The Shadow of Saganami',\n",
       " \"Old Man's War\",\n",
       " 'The Unexpected',\n",
       " 'Catching Fire',\n",
       " 'The World in Winter',\n",
       " 'White Light',\n",
       " 'The Telling',\n",
       " 'Dawn',\n",
       " 'Dune: The Machine Crusade',\n",
       " 'The Book of Dreams',\n",
       " 'Hex',\n",
       " 'Force Heretic: Reunion',\n",
       " 'Red Alert',\n",
       " 'Hammered',\n",
       " 'Heartless',\n",
       " 'Jennie',\n",
       " 'Breakfast of Champions',\n",
       " 'Clade',\n",
       " 'Player Piano',\n",
       " 'Man of Many Minds',\n",
       " 'The Dream Millennium',\n",
       " \"Agents of Chaos: Hero's Trial\",\n",
       " 'Rogue in Space',\n",
       " 'The Sunrise Lands',\n",
       " 'Danny Dunn Scientific Detective',\n",
       " 'Labyrinth of Evil',\n",
       " 'Loose Ends',\n",
       " 'Far Rainbow',\n",
       " 'The Forever War',\n",
       " 'Boba Fett: Maze Of Deception',\n",
       " 'Mass Effect: Retribution',\n",
       " 'Black Fire',\n",
       " 'Dreadnought!',\n",
       " 'The Apocalypse Troll',\n",
       " 'The Goddess of Ganymede',\n",
       " 'Spook Country',\n",
       " 'The Müller-Fokker Effect',\n",
       " 'The Green Odyssey',\n",
       " 'Earth Unaware',\n",
       " 'The Sleeper Awakes',\n",
       " 'Sun of Suns',\n",
       " 'Master of the Void',\n",
       " \"Drunkard's Walk\",\n",
       " 'The Nightmare of Black Island',\n",
       " 'The Final Circle of Paradise',\n",
       " 'Danny Dunn and the Universal Glue',\n",
       " 'The Magic Labyrinth',\n",
       " 'Worldwar: Upsetting the Balance',\n",
       " 'The Giver',\n",
       " 'Escape to Last Man Peak',\n",
       " 'Made of Steel',\n",
       " 'Atlas Shrugged',\n",
       " 'Fire Time',\n",
       " 'Second Foundation',\n",
       " 'The Last Disaster',\n",
       " 'Galaxy of Fear: The Hunger',\n",
       " 'The Philosophical Corps',\n",
       " 'The Golden Age',\n",
       " 'Gears of War: Anvil Gate',\n",
       " 'The Resurrection Casket',\n",
       " 'The Blue Man',\n",
       " 'Destination: Void',\n",
       " 'Return to Mars',\n",
       " 'Beggars and Choosers',\n",
       " 'Fire in the Abyss',\n",
       " 'The Swarm',\n",
       " 'Infernal Devices',\n",
       " 'The Last of the Immortals',\n",
       " 'The World Below',\n",
       " 'The Forbidden Garden',\n",
       " 'Rocheworld',\n",
       " 'The Hidden',\n",
       " 'Buddy Holly is Alive and Well on Ganymede',\n",
       " 'Dreams of Empire',\n",
       " 'World of Ptavvs',\n",
       " 'Empire Star',\n",
       " 'Fire on the Mountain',\n",
       " 'Wetware',\n",
       " 'Assassins',\n",
       " 'Resistance: The Gathering Storm',\n",
       " 'Shards of Honor',\n",
       " 'The Planet of Youth',\n",
       " 'The Keeping Place',\n",
       " 'Ballroom of the Skies',\n",
       " 'Slapstick',\n",
       " 'Star Quest',\n",
       " 'The Fur',\n",
       " 'The Ghost Brigades',\n",
       " 'Kangaroo Notebook',\n",
       " 'The Cat Who Walks Through Walls',\n",
       " 'The Warning',\n",
       " 'Genesis Alpha',\n",
       " 'Thrice Upon a Time',\n",
       " 'A Web of Air',\n",
       " 'The Last of the Jedi: Return of the Dark Side',\n",
       " 'The Deception',\n",
       " 'Knights of Forty Islands',\n",
       " 'Hangfire',\n",
       " 'The Man Who Folded Himself',\n",
       " 'When the People Fell',\n",
       " 'Homunculus',\n",
       " 'The Snow',\n",
       " 'Between the Strokes of Night',\n",
       " 'The War Machine',\n",
       " 'Across The Zodiac',\n",
       " 'The Last of the Jedi: Secret Weapon',\n",
       " \"The Hitchhiker's Guide to the Galaxy\",\n",
       " \"City at World's End\",\n",
       " 'Demon',\n",
       " 'The Invincible',\n",
       " 'The Fountains of Paradise',\n",
       " 'Perdido Street Station',\n",
       " 'Vengeance of Orion',\n",
       " 'Gridiron',\n",
       " 'Home Is the Hunter',\n",
       " 'The Positronic Man',\n",
       " 'In the Days of the Comet',\n",
       " 'The Rising Force',\n",
       " 'The Glass Bees',\n",
       " 'Thorns',\n",
       " 'Nearly Neptune',\n",
       " 'Le père de nos pères',\n",
       " 'The Outlaws of Mars',\n",
       " 'Breakpoint',\n",
       " 'Code of the Lifemaker',\n",
       " 'Resistance',\n",
       " 'Babylon 5: Legions of Fire - Armies of Light and Dark',\n",
       " 'Outlander',\n",
       " 'Dying in the Sun',\n",
       " '3001: The Final Odyssey',\n",
       " 'Market Forces',\n",
       " 'Out of the Silent Planet',\n",
       " 'Night of the Big Heat',\n",
       " \"Isaac Asimov's Utopia\",\n",
       " 'Specter of the Past',\n",
       " 'Grave Matter',\n",
       " 'Showboat World',\n",
       " 'Gray Matters',\n",
       " 'When the Wind Blows',\n",
       " 'City on Fire',\n",
       " 'Trapped',\n",
       " 'The Intergalactic Kitchen',\n",
       " 'The Forge of God',\n",
       " 'Boneshaker',\n",
       " 'The Moon Maiden',\n",
       " 'Abduction',\n",
       " 'The Concrete Blonde',\n",
       " 'The Only Witness',\n",
       " 'Inferno',\n",
       " 'The House of the Scorpion',\n",
       " 'The World Wreckers',\n",
       " 'The Zenith Angle',\n",
       " 'Running from the Deity',\n",
       " 'The First Men in the Moon',\n",
       " 'Star Wars Jedi Quest 1: The Way of the Apprentice',\n",
       " 'Nebula Maker',\n",
       " 'The Secret of Sinharat',\n",
       " 'Expendable',\n",
       " 'Off On A Comet',\n",
       " 'Aggressor Six',\n",
       " 'Terminal World',\n",
       " 'The Carbon Diaries: 2015',\n",
       " 'Singularity Sky',\n",
       " 'Heart of TARDIS',\n",
       " 'The White Plague',\n",
       " 'A Door Into Ocean',\n",
       " 'The Voyage of the Space Beagle',\n",
       " 'Earth Revisited',\n",
       " 'The Far Arena',\n",
       " 'Tea from an Empty Cup',\n",
       " 'Mona Lisa Overdrive',\n",
       " 'Facing the Flag',\n",
       " 'First Light',\n",
       " 'The Forlorn Hope',\n",
       " 'The Road',\n",
       " \"Surak's Soul\",\n",
       " 'Tactics of Mistake',\n",
       " 'Santa Olivia',\n",
       " 'Flowers for Algernon',\n",
       " 'Face of the Enemy',\n",
       " 'The Swords of Zinjaban',\n",
       " 'Richter 10',\n",
       " 'Mission to Magnus',\n",
       " 'Hitman: Enemy Within',\n",
       " \"The Time Traveler's Wife\",\n",
       " 'The Secret of the League',\n",
       " '1636: The Saxon Uprising',\n",
       " 'Lurulu',\n",
       " 'Matter',\n",
       " 'The Truth Machine',\n",
       " 'My Name is Legion',\n",
       " 'Cerberus: A Wolf in the Fold',\n",
       " 'The Lost Fleet: Valiant',\n",
       " 'Pattern Recognition',\n",
       " 'The Extremes',\n",
       " 'The Clone Wars',\n",
       " 'Mother Night',\n",
       " 'Ada or Ardor: A Family Chronicle',\n",
       " \"Lion's Blood\",\n",
       " 'Jumper',\n",
       " 'The Traveler',\n",
       " \"All Tomorrow's Parties\",\n",
       " 'The Ice Limit',\n",
       " 'The Wild Boy',\n",
       " 'A Practical Man',\n",
       " 'Demons',\n",
       " 'Ecotopia',\n",
       " 'Household Gods',\n",
       " 'Eternity Road',\n",
       " 'The Godmakers',\n",
       " 'Settling Accounts: Drive to the East',\n",
       " 'A Planet Called Treason',\n",
       " 'The Flying Sorcerers',\n",
       " 'Brothers of Earth',\n",
       " 'The Dolphins of Pern',\n",
       " 'Paul of Dune',\n",
       " 'The Illusion',\n",
       " 'The Fatal Eggs',\n",
       " 'Contact',\n",
       " 'Suicide Circle: The Complete Edition',\n",
       " 'Marooned in Realtime',\n",
       " 'Placebo Effect',\n",
       " \"Anno Domini 2000 - A Woman's Destiny\",\n",
       " 'Genesis',\n",
       " 'G.O.G. 666',\n",
       " 'Stone',\n",
       " 'Martians, Go Home',\n",
       " 'Rocket to Luna',\n",
       " 'Spaceship to Saturn',\n",
       " 'Blast Off at Woomera',\n",
       " 'Night Lamp',\n",
       " 'Interstellar Pig',\n",
       " 'The Purchase of the North Pole',\n",
       " 'Battlestations!',\n",
       " 'The World Before',\n",
       " 'The Supernaturalist',\n",
       " 'Digital Fortress',\n",
       " 'The Witches of Karres',\n",
       " 'Wing Commander: False Colors',\n",
       " 'What Mad Universe',\n",
       " 'Ishmael',\n",
       " 'Sunrise Alley',\n",
       " 'Tripoint',\n",
       " '2061: Odyssey Three',\n",
       " 'Bold as Love',\n",
       " 'The Master of Disguise',\n",
       " 'The Labyrinth Key',\n",
       " 'Imperial Earth',\n",
       " 'Fiasco',\n",
       " 'Metro 2033',\n",
       " 'Against the Tide of Years',\n",
       " 'Where Late the Sweet Birds Sang',\n",
       " 'Dark Carnival',\n",
       " 'Sinister Barrier',\n",
       " 'Speech Sounds',\n",
       " 'Genus Homo',\n",
       " 'The Dark Volume',\n",
       " 'Jennifer Government',\n",
       " 'Have Space Suit-Will Travel',\n",
       " 'Tik-Tok',\n",
       " 'Mind Changer',\n",
       " 'The Other Place',\n",
       " 'Downward to the Earth',\n",
       " 'The Glass Bead Game',\n",
       " 'The Stones of Nomuru',\n",
       " 'Nemesis',\n",
       " 'Wasp',\n",
       " 'Time out of Joint',\n",
       " 'Star Wars: Darth Bane: Dynasty of Evil',\n",
       " 'The Whole Man',\n",
       " 'The Star-Crowned Kings',\n",
       " 'The Indestructible Man',\n",
       " 'As She Climbed Across the Table',\n",
       " 'Warlords of Utopia',\n",
       " 'The Wounded Sky',\n",
       " 'The Free Lunch',\n",
       " 'Night of Light',\n",
       " 'I Am Number Four',\n",
       " 'Mission of Honor',\n",
       " 'Calculating God',\n",
       " 'Brain Wave',\n",
       " \"Newton's Wake: A Space Opera\",\n",
       " 'The Solution',\n",
       " 'Blade Runner 3: Replicant Night',\n",
       " '1635: The Tangled Web',\n",
       " 'Clans of the Alphane Moon',\n",
       " 'Flatland: A Romance of Many Dimensions',\n",
       " 'Resurrection Day',\n",
       " 'Plague Ship',\n",
       " 'Blasphemy',\n",
       " \"Logan's Run\",\n",
       " 'Autonomy',\n",
       " 'First Contact',\n",
       " 'Sliding Scales',\n",
       " 'Still Forms on Foxfield',\n",
       " 'False Mirrors',\n",
       " 'Veracity',\n",
       " 'The Unicorn Girl',\n",
       " 'Telempath',\n",
       " 'Betrayal',\n",
       " 'Devil on My Back',\n",
       " 'Salt',\n",
       " 'The Duplicate',\n",
       " 'Babylon Babies',\n",
       " 'Tin Woodman',\n",
       " \"The Bull's Hour\",\n",
       " 'Impact',\n",
       " 'Exile',\n",
       " 'Flashforward',\n",
       " 'Savage Pellucidar',\n",
       " 'Dwellers in the Crucible',\n",
       " 'After Doomsday',\n",
       " 'Shadow',\n",
       " 'The Unknown',\n",
       " 'Designated Targets',\n",
       " 'The Death Of Grass',\n",
       " 'Elvissey',\n",
       " 'Typewriter in the Sky',\n",
       " 'Death Troopers',\n",
       " \"Junior Jedi Knights: Vader's Fortress\",\n",
       " 'Iron Council',\n",
       " 'Cloak of Deception',\n",
       " 'Mass Effect: Ascension',\n",
       " 'Starclimber',\n",
       " 'Robur the Conqueror',\n",
       " 'Moon Base One',\n",
       " \"Vulcan's Glory\",\n",
       " 'Warring States',\n",
       " \"Scrivener's Moon\",\n",
       " 'Making History',\n",
       " 'Emergence',\n",
       " 'Eon',\n",
       " 'Fear of the Dark',\n",
       " 'The Land Leviathan',\n",
       " \"The Romulan War: Beneath the Raptor's Wing\",\n",
       " 'The Loch',\n",
       " 'Transit',\n",
       " 'Myron',\n",
       " 'Damnation Alley',\n",
       " 'The Creator',\n",
       " 'The Master: An Adventure Story',\n",
       " 'Danny Dunn and the Swamp Monster',\n",
       " 'Lanark: A Life in Four Books',\n",
       " 'The Glove of Darth Vader',\n",
       " 'Solaris',\n",
       " 'Jedi Trial',\n",
       " 'The Dark Triangle',\n",
       " 'Mindplayers',\n",
       " 'The Independent Command',\n",
       " 'Patrimony',\n",
       " 'Shadrach in the Furnace',\n",
       " 'Swastika Night',\n",
       " 'A Crystal Age',\n",
       " 'The Restaurant at the End of the Universe',\n",
       " 'Antarctica',\n",
       " 'Uglies',\n",
       " 'A Matter of Profit',\n",
       " 'Taronga',\n",
       " 'The Dark Design',\n",
       " 'Specials',\n",
       " 'To the Stars',\n",
       " 'We Who Are About To...',\n",
       " 'The Mohole Mystery',\n",
       " 'Ammonite',\n",
       " 'Blind Lake',\n",
       " 'Redemption Ark',\n",
       " 'Things That Are',\n",
       " 'Cold Fusion',\n",
       " 'Shield of Lies',\n",
       " 'The Scales of Injustice',\n",
       " 'Hilldiggers',\n",
       " 'Millennium Falcon',\n",
       " 'Starman Jones',\n",
       " 'The Experiment',\n",
       " 'The Dreaming Void',\n",
       " 'Earthworks',\n",
       " 'Conqueror',\n",
       " 'Warlord of the Air',\n",
       " 'The World Inside',\n",
       " 'Hot Sleep',\n",
       " 'Adiamante',\n",
       " 'Deathstalker',\n",
       " 'Or Die Trying',\n",
       " 'Angel Island',\n",
       " 'The Shadow in the Glass',\n",
       " 'Ladies Whose Bright Eyes',\n",
       " \"Building Harlequin's Moon\",\n",
       " 'Echo Round His Bones',\n",
       " 'The Miocene Arrow',\n",
       " 'The Gap in the Curtain',\n",
       " 'To Venus in Five Seconds',\n",
       " 'The Integral Trees',\n",
       " 'My Enemy, My Ally',\n",
       " 'Secrets of the Jedi',\n",
       " 'Ilium/Olympus',\n",
       " '1633',\n",
       " 'Address: Centauri',\n",
       " 'A War of Gifts: An Ender Story',\n",
       " 'Genome',\n",
       " 'Shockwave',\n",
       " 'The Number of the Beast',\n",
       " 'Corpse Marker',\n",
       " 'The Planiverse',\n",
       " 'Ilium',\n",
       " 'Courtship Rite',\n",
       " \"Ringworld's Children\",\n",
       " 'Star Light',\n",
       " 'Last Son of Krypton',\n",
       " 'Saturnalia',\n",
       " \"Daedalus's Children\",\n",
       " 'Marooned on Mars',\n",
       " 'The Capture',\n",
       " 'Fear Itself',\n",
       " 'A Dream of Wessex',\n",
       " 'The City of Ember',\n",
       " 'The Gray Cloth',\n",
       " 'Sargasso of Space',\n",
       " 'True History',\n",
       " 'Between Planets',\n",
       " 'The Ask and the Answer',\n",
       " 'Time for Yesterday',\n",
       " '1635: The Dreeson Incident',\n",
       " 'The Shrouded Planet',\n",
       " 'The Difference Engine',\n",
       " 'The Prince',\n",
       " 'Concrete Island',\n",
       " \"The Centurion's Empire\",\n",
       " 'Mission to Mercury',\n",
       " 'On Wings of Song',\n",
       " 'Tribulation Force',\n",
       " \"Starman's Quest\",\n",
       " 'Ellimist Chronicles',\n",
       " 'The Bowl of Baal',\n",
       " 'The Maze Runner',\n",
       " 'Pursuit of the Screamer',\n",
       " 'Children of The Dust',\n",
       " 'Prisoners of Power',\n",
       " 'Forty Signs of Rain',\n",
       " 'Murder in Millennium VI',\n",
       " 'The Postman',\n",
       " 'Acts of God',\n",
       " 'Danny Dunn on a Desert Island',\n",
       " 'Fusion Fire',\n",
       " 'Dune: The Battle of Corrin',\n",
       " 'After the Bomb',\n",
       " 'Heirs of Empire',\n",
       " 'Colossus and the Crab',\n",
       " 'Beetle in the Anthill',\n",
       " 'Leviathan',\n",
       " 'The Bladerunner',\n",
       " 'Danny Dunn and the Smallifying Machine',\n",
       " 'The Glory That Was',\n",
       " 'Spherical Harmonic',\n",
       " 'Dreamsnake',\n",
       " 'Invasive Procedures',\n",
       " 'Line of Delirium',\n",
       " 'Lód',\n",
       " 'Just War',\n",
       " 'Dying of the Light',\n",
       " 'The Price of Paradise',\n",
       " 'The Kobayashi Maru',\n",
       " 'Children of the Jedi',\n",
       " 'Drift',\n",
       " 'Rasputin',\n",
       " 'The Songs of Distant Earth',\n",
       " 'The Master Mind of Mars',\n",
       " 'The Year of the Angry Rabbit',\n",
       " 'Daybreak Zero',\n",
       " 'Orion in the Dying Time',\n",
       " 'Needle',\n",
       " 'The Fourth \"R\"',\n",
       " 'Subspace Encounter',\n",
       " 'The Ghosts of N-Space',\n",
       " 'Prime Time',\n",
       " 'Dark Tide: Ruin',\n",
       " 'Babylon 5: Dark Genesis - The Birth of the Psi Corps',\n",
       " '...And Call Me Conrad',\n",
       " 'The Unifying Force',\n",
       " 'American Empire: The Victorious Opposition',\n",
       " 'Falling Free',\n",
       " 'Little Green Men: A Novel',\n",
       " 'Destination Mars',\n",
       " 'OtherSpace',\n",
       " \"Dragon's Egg\",\n",
       " 'Monument',\n",
       " 'The Infinity Doctors',\n",
       " 'Rogue',\n",
       " 'Robots and Empire',\n",
       " 'The Eye of the Giant',\n",
       " 'Sphere',\n",
       " 'The Dark Rival',\n",
       " 'Automated Alice',\n",
       " 'The Black Cloud',\n",
       " 'The Absolute',\n",
       " 'The Inheritors',\n",
       " 'Babel-17',\n",
       " 'Apollyon',\n",
       " 'The Three Stigmata of Palmer Eldritch',\n",
       " 'The Absolute at Large',\n",
       " 'The Secret People',\n",
       " 'Blade Runner 4: Eye and Talon',\n",
       " 'Starcross',\n",
       " 'The Worst Band In The Universe',\n",
       " '.hack//Another Birth',\n",
       " 'Prophets of the Dark Side',\n",
       " 'The Robots of Dawn',\n",
       " 'Eager',\n",
       " 'Empty World',\n",
       " 'The Rat Race',\n",
       " 'Final Draft',\n",
       " 'A Flag Full of Stars',\n",
       " 'In the Ocean of Night',\n",
       " 'The Lost City of the Jedi',\n",
       " 'Midnighters 2: Touching Darkness',\n",
       " 'Invitation to the Game',\n",
       " 'Drowning World',\n",
       " 'The Monsters Inside',\n",
       " 'Our Friends from Frolix 8',\n",
       " 'The Feast of the Drowned',\n",
       " 'The State of the Art',\n",
       " 'Riddley Walker',\n",
       " 'Minions of the Moon',\n",
       " 'Turnabout',\n",
       " 'Galactic Odyssey',\n",
       " 'The Stories of Ibis',\n",
       " 'To Say Nothing of the Dog',\n",
       " 'Doorways in the Sand',\n",
       " 'The Iron Heel',\n",
       " 'The Predator',\n",
       " 'Buck Rogers: A Life in the Future',\n",
       " 'Trouble with Lichen',\n",
       " 'Terraplane',\n",
       " 'Crisis on Centaurus',\n",
       " 'Operation Nuke',\n",
       " 'When Worlds Collide',\n",
       " 'The Peacekeepers',\n",
       " 'The Prophecy',\n",
       " 'The Last of the Jedi: Against the Empire',\n",
       " 'The Futurological Congress',\n",
       " 'The Angel of the Revolution',\n",
       " 'Excession',\n",
       " 'Counter-Clock World',\n",
       " 'Captain Underpants and the Wrath of the Wicked Wedgie Woman',\n",
       " 'Terminal Freeze',\n",
       " 'Century Rain',\n",
       " 'Chain of Attack',\n",
       " 'Lord of the Trees',\n",
       " 'The Sentimental Agents in the Volyen Empire',\n",
       " 'Halo: The Fall of Reach',\n",
       " 'The Revelation',\n",
       " 'Bloodthirst',\n",
       " 'With Red Hands',\n",
       " 'Mind Switch',\n",
       " 'Space Platform',\n",
       " \"Jumper: Griffin's Story\",\n",
       " 'The Lake House',\n",
       " 'World Without End',\n",
       " 'Equinox',\n",
       " 'The Legion of Space',\n",
       " 'Teranesia',\n",
       " 'The Burning',\n",
       " 'Alice, Girl from the Future',\n",
       " 'The Forgotten Planet',\n",
       " 'Mortal Engines',\n",
       " 'The Invention of Morel',\n",
       " 'Moon of Mutiny',\n",
       " 'Spin',\n",
       " 'First Contact?',\n",
       " 'The Discovery',\n",
       " 'Matriarch',\n",
       " 'The Last of the Jedi: Death on Naboo',\n",
       " 'Cyborg IV',\n",
       " 'Cagebird',\n",
       " 'Spaceship Medic',\n",
       " 'The Scourge of God',\n",
       " 'The Sun Saboteurs',\n",
       " 'Flatterland',\n",
       " 'Triangle',\n",
       " 'The Blind Spot',\n",
       " 'Observation on the Spot',\n",
       " 'Cradle',\n",
       " 'Storm Thief',\n",
       " 'Ports of Call',\n",
       " 'The Book of Dave',\n",
       " 'The Atom Clock',\n",
       " 'Earthlight',\n",
       " 'The Sacrifice',\n",
       " 'Half Past Human',\n",
       " 'Footfall',\n",
       " 'Ranks of Bronze',\n",
       " 'The Andalite Chronicles',\n",
       " 'The Time Ships',\n",
       " 'The Sickness',\n",
       " 'Mission: Impractical',\n",
       " 'The True Meaning of Smekday',\n",
       " 'Fantastic Voyage II: Destination Brain',\n",
       " 'Eye of Cat',\n",
       " 'Pattern for Conquest',\n",
       " 'People of the Comet',\n",
       " 'Command & Conquer: Tiberium Wars',\n",
       " 'Camp Concentration',\n",
       " 'The Stone Key',\n",
       " 'Girl in Landscape',\n",
       " 'A Rising Thunder',\n",
       " 'The New Rebellion',\n",
       " 'Band of Gypsys',\n",
       " 'Necroscope III: The Source',\n",
       " 'The Scarlet Empire',\n",
       " 'The Shockwave Rider',\n",
       " 'Weapons of Choice',\n",
       " 'Greatheart Silver',\n",
       " 'Autumn Visits',\n",
       " 'The Indwelling',\n",
       " 'That Hideous Strength',\n",
       " 'Extras',\n",
       " 'Encounter With Tiber',\n",
       " 'The Suspicion',\n",
       " 'A for Anything',\n",
       " 'Incandescence',\n",
       " 'Under the Skin',\n",
       " 'Johnny Mackintosh and the Spirit of London',\n",
       " 'The Return',\n",
       " 'The Reaction',\n",
       " 'The Windup Girl',\n",
       " 'The Caves of Drach',\n",
       " 'The Seer',\n",
       " 'Mahars of Pellucidar',\n",
       " 'A Strange Manuscript Found in a Copper Cylinder',\n",
       " 'Dhalgren',\n",
       " 'Rainbows End',\n",
       " 'Giles Goat-Boy',\n",
       " 'Simulacron-3',\n",
       " 'Waiting for the Galactic Bus',\n",
       " 'The Scorch Trials',\n",
       " 'The Quiet Earth',\n",
       " 'To Live Again',\n",
       " 'Wither',\n",
       " 'Titan',\n",
       " 'The Joiner King',\n",
       " '334',\n",
       " 'Tempest',\n",
       " 'The Evolutionary Void',\n",
       " 'Children of God',\n",
       " 'Voyage from Yesteryear',\n",
       " 'Toki o Kakeru Shōjo',\n",
       " 'Down and Out in the Magic Kingdom',\n",
       " 'The Descent of Anansi',\n",
       " 'Boba Fett: A New Threat',\n",
       " 'The Stone Rose',\n",
       " 'Parable of the Sower',\n",
       " 'Chaos and Order',\n",
       " 'Blameless',\n",
       " 'The Armageddon Rag',\n",
       " 'The Diothas',\n",
       " 'Caballo de Troya',\n",
       " 'Brightness Reef',\n",
       " 'Tunnel in the Sky',\n",
       " 'Prey',\n",
       " 'City of Pearl',\n",
       " 'Gravity Dreams',\n",
       " 'The Naked Sun',\n",
       " 'Rocket Ship Galileo',\n",
       " 'The Second Invasion from Mars',\n",
       " 'Cinder',\n",
       " 'The Land of Crimson Clouds',\n",
       " 'Stranger in a Strange Land',\n",
       " 'Sten Adventures Book 5: Revenge of the Damned',\n",
       " 'Halo: Contact Harvest',\n",
       " 'Space Tug',\n",
       " 'Conjure Wife',\n",
       " \"Stranglers' Moon\",\n",
       " 'The Encounter',\n",
       " 'The Quantum Archangel',\n",
       " 'The Man in the Moone',\n",
       " 'The Years of Rice and Salt',\n",
       " 'Survivors',\n",
       " 'The Hacker and the Ants',\n",
       " 'Labyrinth of Reflections',\n",
       " '1634: The Ram Rebellion',\n",
       " 'Confessions of a Crap Artist',\n",
       " \"Great Kings' War\",\n",
       " 'Machine Man',\n",
       " 'The Good That Men Do',\n",
       " 'The Death Guard',\n",
       " 'Icefire',\n",
       " \"Nerilka's Story\",\n",
       " 'The Last Space Viking',\n",
       " 'Spaceland',\n",
       " 'The Final Sanction',\n",
       " 'Semper Mars',\n",
       " 'Hard to Be a God',\n",
       " 'Herland',\n",
       " 'Danny Dunn and the Fossil Cave',\n",
       " 'One in Three Hundred',\n",
       " 'Rendezvous with Rama',\n",
       " 'Synthespians™',\n",
       " 'The Night of Kadar',\n",
       " 'Count Zero',\n",
       " 'Sphereland',\n",
       " 'The Call to Vengeance',\n",
       " 'The Quantum Thief',\n",
       " 'Man of Two Worlds',\n",
       " 'Dayworld',\n",
       " 'Hestia',\n",
       " 'Proteus in the Underworld',\n",
       " 'The Other',\n",
       " 'The Rise of Nine',\n",
       " 'Dragonheart',\n",
       " 'Edge of Victory: Conquest',\n",
       " 'A Maze of Death',\n",
       " 'The Mark of the Crown',\n",
       " 'Singing the Dogstar Blues',\n",
       " 'The Sword of Aldones',\n",
       " 'Dune',\n",
       " 'The Roundheads',\n",
       " 'The Space Vampires',\n",
       " 'Dragonsblood',\n",
       " 'The Excalibur Alternative',\n",
       " 'The Ear, the Eye, and the Arm',\n",
       " 'The Message',\n",
       " 'The Third Lynx',\n",
       " 'Rising Sun',\n",
       " 'Force Heretic: Refugee',\n",
       " 'The Shape of Things to Come',\n",
       " 'The Mark',\n",
       " 'Puttering About in a Small Land',\n",
       " 'The Knife of Never Letting Go',\n",
       " 'More Than Human',\n",
       " 'Moving the Mountain',\n",
       " 'Doon',\n",
       " 'Sender Unknown',\n",
       " 'Cloak',\n",
       " 'Under the Triple Suns',\n",
       " 'Bend Sinister',\n",
       " 'Transit to Scorpio',\n",
       " 'The Right to Arm Bears',\n",
       " 'The Mutation',\n",
       " 'Weapon',\n",
       " 'Expedition Venus',\n",
       " 'Icehenge',\n",
       " 'Land of the Headless',\n",
       " 'Palos of the Dog Star Pack',\n",
       " 'The Man in the Maze',\n",
       " 'Q-Squared',\n",
       " 'The Hammer of God',\n",
       " 'Sister Alice',\n",
       " 'Fallen Dragon',\n",
       " 'The Exile Kiss',\n",
       " 'Star Wars Republic Commando: Hard Contact',\n",
       " 'A Mystery for Mr. Bass',\n",
       " 'Oath of Fealty',\n",
       " 'The Tomorrow People',\n",
       " 'Verdigris',\n",
       " 'The Weakness',\n",
       " 'Manifold: Space',\n",
       " 'Sent',\n",
       " 'Seeker',\n",
       " \"Christie Malry's Own Double-Entry\",\n",
       " 'Mockingjay',\n",
       " 'Intermere',\n",
       " 'Junior Jedi Knights: The Golden Globe',\n",
       " 'Who?',\n",
       " 'Be More Chill',\n",
       " 'Enemy Lines: Rebel Stand',\n",
       " 'Danny Dunn and the Heat Ray',\n",
       " 'The Gripping Hand',\n",
       " 'Among the Impostors',\n",
       " 'Under the Yoke',\n",
       " 'There Will Be Time',\n",
       " 'God Emperor of Dune',\n",
       " 'The Anubis Gates',\n",
       " 'Time Zero',\n",
       " 'From the Earth to the Moon',\n",
       " 'City',\n",
       " 'Songmaster',\n",
       " 'Forbidden Knowledge',\n",
       " 'Earth Made of Glass',\n",
       " 'The Cyborg from Earth',\n",
       " 'The Jesus Incident',\n",
       " \"The Protector's War\",\n",
       " 'The Keeper of the Isis Light',\n",
       " 'The Sons of Heaven',\n",
       " 'The Face of the Waters',\n",
       " 'Big Planet',\n",
       " 'Seven Days in New Crete',\n",
       " 'The Swarm War',\n",
       " 'Quicksilver',\n",
       " 'Land of Terror',\n",
       " 'The Fifth Man',\n",
       " 'The Outward Urge',\n",
       " 'Operation: Outer Space',\n",
       " 'A Descent into the Maelstrom',\n",
       " 'The Ashes of Eden',\n",
       " 'Fahrenheit 451',\n",
       " 'Children of the Atom',\n",
       " 'The Icarus Hunt',\n",
       " 'Danny Dunn and the Anti-Gravity Paint',\n",
       " 'The Word',\n",
       " 'Ravage',\n",
       " 'Behind Enemy Lines',\n",
       " 'Petrogypsies',\n",
       " 'Engine Summer',\n",
       " 'Darksaber',\n",
       " 'The Rithian Terror',\n",
       " 'Moonseed',\n",
       " 'Professor Shonku',\n",
       " 'The Santaroga Barrier',\n",
       " 'Monsters of Men',\n",
       " 'Deeper',\n",
       " 'Man After Man: An Anthropology of the Future',\n",
       " 'Star Wars Imperial Commando: 501st',\n",
       " 'Soulless',\n",
       " 'The Flames: A Fantasy',\n",
       " 'Exultant',\n",
       " 'The Children of the Company',\n",
       " 'The Memory of Earth',\n",
       " 'The Hunger Games',\n",
       " 'Forward the Foundation',\n",
       " 'Jason, Son of Jason',\n",
       " 'Through Violet Eyes',\n",
       " 'The Romulan Way',\n",
       " 'TIM Defender of the Earth',\n",
       " 'Spectrum',\n",
       " \"Callahan's Lady\",\n",
       " 'The Beast Master',\n",
       " 'Brothers in Arms',\n",
       " 'The Astronauts',\n",
       " 'Ralph 124C 41+',\n",
       " 'The Dirdir',\n",
       " 'Primary Inversion',\n",
       " 'Mach 1: A Story of Planet Ionus',\n",
       " 'Northworld',\n",
       " 'Mendoza in Hollywood',\n",
       " 'The Weapon Shops of Isher',\n",
       " \"Gravity's Rainbow\",\n",
       " \"Lord Kelvin's Machine\",\n",
       " 'The Young Unicorns',\n",
       " 'The Crystal World',\n",
       " 'Murder on Mars',\n",
       " 'The Stone Dogs',\n",
       " 'A Million Open Doors',\n",
       " 'Marching Through Georgia',\n",
       " 'Festival of Death',\n",
       " 'The Last Children of Schewenborn',\n",
       " 'Shikasta',\n",
       " 'The Bird of Time',\n",
       " 'The Wheels of Chance',\n",
       " 'Mathematicians in Love',\n",
       " 'Pirates of Venus',\n",
       " 'Anthem',\n",
       " \"Farnham's Freehold\",\n",
       " 'Rogue Moon',\n",
       " 'Candle',\n",
       " 'La Nuit des temps',\n",
       " 'Across the Sea of Suns',\n",
       " 'The Resistance',\n",
       " \"Serpent's Reach\",\n",
       " \"Edison's Conquest of Mars\",\n",
       " 'The Ruby Dice',\n",
       " 'The Attack',\n",
       " 'The Quiet Game',\n",
       " 'Learning the World',\n",
       " 'Non-Stop',\n",
       " 'Invincible',\n",
       " 'Dark Force Rising',\n",
       " 'Pandemia',\n",
       " 'Where were you last Pluterday?',\n",
       " 'Northern Lights',\n",
       " \"Tyrant's Test\",\n",
       " \"Survivor's Quest\",\n",
       " 'The World is Round',\n",
       " 'The Call of Earth',\n",
       " 'Accelerando',\n",
       " 'Alas, Babylon',\n",
       " 'Crown of Slaves',\n",
       " 'The Remnant',\n",
       " 'Out of Time’s Abyss',\n",
       " 'Birth of Fire',\n",
       " 'Cryoburn',\n",
       " 'Michaelmas',\n",
       " 'Decipher',\n",
       " 'Path of Unreason',\n",
       " 'Kindred',\n",
       " 'The Venom Trees of Sunga',\n",
       " 'All You Need Is Kill',\n",
       " 'Return from the Stars',\n",
       " 'The Last Aerie',\n",
       " 'State of Fear',\n",
       " 'The Power of Six',\n",
       " 'Macrolife',\n",
       " 'Rama II',\n",
       " 'Among the Brave',\n",
       " 'The Clockwise Man',\n",
       " 'Tower of Glass',\n",
       " 'Star Wars: Crosscurrent',\n",
       " 'The Coming of the Terraphiles',\n",
       " 'Daniel X: Watch the Skies',\n",
       " '11/22/63',\n",
       " 'Timeline',\n",
       " '2030',\n",
       " 'Island in the Sea of Time',\n",
       " 'Rama Revealed',\n",
       " 'The Hidden Past',\n",
       " 'The Forest of Hands and Teeth',\n",
       " 'Black Man',\n",
       " 'Navigator',\n",
       " 'Star Wars Republic Commando: True Colors',\n",
       " 'The Night People',\n",
       " 'Fight Club',\n",
       " 'The Ships of Earth',\n",
       " \"The Begum's Millions\",\n",
       " 'The Strange World of Planet X',\n",
       " 'Ashes of Victory',\n",
       " 'Lords of the Starship',\n",
       " 'Dark Mirror',\n",
       " 'Empire of the Atom',\n",
       " 'Stowaway to the Mushroom Planet',\n",
       " 'A Meeting at Corvallis',\n",
       " 'Abyss',\n",
       " 'Neuromancer',\n",
       " 'Galaxy of Fear: The Doomsday Ship',\n",
       " \"Mutineers' Moon\",\n",
       " 'Rider at the Gate',\n",
       " 'With the Lightnings',\n",
       " 'Sakkara',\n",
       " 'Sacrifice',\n",
       " 'Truancy',\n",
       " 'Hover Car Racer',\n",
       " 'Battle Royale',\n",
       " 'Half a Crown',\n",
       " 'Land Beyond the Map',\n",
       " 'The IDIC Epidemic',\n",
       " 'The Final Nexus',\n",
       " 'The Search For Snout',\n",
       " \"Evening's Empire\",\n",
       " \"Marco's millions\",\n",
       " 'The Life of the World to Come',\n",
       " 'Remnant Population',\n",
       " 'The Rolling Stones',\n",
       " 'Equality',\n",
       " 'Runaway',\n",
       " 'The Visitors',\n",
       " 'The Other Log of Phileas Fogg',\n",
       " 'The Troika',\n",
       " 'After Worlds Collide',\n",
       " 'The Sunken World',\n",
       " 'Man of Earth',\n",
       " 'Nymphomation',\n",
       " 'Flashfire',\n",
       " 'A Woman of the Iron People',\n",
       " 'Metal Fatigue',\n",
       " 'The Mightiest Machine',\n",
       " '1635: The Cannon Law',\n",
       " 'The Man Who Awoke',\n",
       " 'Planet X',\n",
       " 'The Pretender',\n",
       " 'Junior Jedi Knights: Promises',\n",
       " 'Ring',\n",
       " 'The Making of the Representative for Planet 8',\n",
       " 'Killobyte',\n",
       " 'A Gift From Earth',\n",
       " 'Among the Free',\n",
       " 'Abercrombie Station',\n",
       " 'The Nitrogen Fix',\n",
       " 'Down to a Sunless Sea',\n",
       " 'Ann Veronica',\n",
       " 'The Sunless City',\n",
       " 'The Evil Experiment',\n",
       " 'The Man in the High Castle',\n",
       " 'Ceres Storm',\n",
       " 'The Naked God',\n",
       " 'Independence Day',\n",
       " 'Winning Colors',\n",
       " 'Dies the Fire',\n",
       " 'Maza of the Moon',\n",
       " 'Vesper',\n",
       " \"Destiny's Way\",\n",
       " \"Predator's Gold\",\n",
       " 'Lucky Starr and the Pirates of the Asteroids',\n",
       " 'Steel Beach',\n",
       " 'Things Not Seen',\n",
       " 'Star Wars Jedi Quest 8: The Changing of the Guard',\n",
       " 'The Departure',\n",
       " \"Doctor's Orders\",\n",
       " 'Zanesville',\n",
       " 'The Fortunate Fall',\n",
       " 'The Steel Tsar',\n",
       " 'Cyborg',\n",
       " \"Cuckoo's Egg\",\n",
       " \"Sten Adventures Book 8: Empire's End\",\n",
       " 'Highways in Hiding',\n",
       " 'A Dark and Hungry God Arises',\n",
       " 'Colossus',\n",
       " 'Millennium',\n",
       " 'Donnerjack',\n",
       " 'The Computer Connection',\n",
       " 'Shadow of the Giant',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scifi = check[check['genre'] == 'science fiction']\n",
    "set(scifi.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_title(scifi, title):\n",
    "    if scifi['title'].str.contains(title).any():\n",
    "        return 'taken'\n",
    "    else:\n",
    "        return 'available title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available title'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"The Left Hand of Darkness\"\n",
    "check_title(scifi, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Left Hand of Darkness by Ursula K. Le Guin (1969)\n",
    "\n",
    "summary = \"The novel follows the story of Genly Ai, a human native of Terra, who is sent to the planet of Gethen as an envoy of the Ekumen, a loose confederation of planets. Ai's mission is to persuade the nations of Gethen to join the Ekumen, but he is stymied by a lack of understanding of their culture. Individuals on Gethen are ambisexual, with no fixed sex; this has a strong influence on the culture of the planet, and creates a barrier of understanding for Ai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate model for testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "def genre_predictor(new_summary):\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    with open('xgboost.pkl', 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    # New data\n",
    "    new_summary = \"The novel follows the story of Genly Ai, a human native of Terra, who is sent to the planet of Gethen as an envoy of the Ekumen, a loose confederation of planets. Ai's mission is to persuade the nations of Gethen to join the Ekumen, but he is stymied by a lack of understanding of their culture. Individuals on Gethen are ambisexual, with no fixed sex; this has a strong influence on the culture of the planet, and creates a barrier of understanding for Ai.\"\n",
    "    \n",
    "    # Convert new data to the same format as my training data\n",
    "    # TF-IDF vectorization for tokens\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tokens_tfidf = tfidf_vectorizer.fit_transform(check['cleaned_summary'].astype(str))\n",
    "    new_tfid = tfidf_vectorizer.transform([new_summary])\n",
    "    \n",
    "    # Assuming 'new_data_vectorized' is a list of feature vectors for your new data\n",
    "    new_data_array = np.array(new_tfid)\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_data_array)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    for new_summary, predicted_label in zip(new_data, predicted_labels):\n",
    "        print(f\"Summary: {new_summary} | Predicted Genre: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/usuari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download the WordNet lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_and_predict(new_summary, model_path='xgboost.pkl', vectorizer=None):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    # Use the provided vectorizer or create a new one\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the training data\n",
    "    check['preprocessed_summary'] = check['cleaned_summary'].apply(preprocess_text)\n",
    "    \n",
    "    # TF-IDF vectorization for tokens\n",
    "    tokens_tfidf = vectorizer.fit_transform(check['preprocessed_summary'])\n",
    "    \n",
    "    # Apply the same text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    for summary, predicted_label in zip([new_summary], predicted_labels):\n",
    "        print(f\"Summary: {summary} | Predicted Genre: {predicted_label}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# preprocess_and_predict(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_predict_2(new_summary, model_path='xgboost.pkl', vectorizer_path='tfidf_vectorizer.pkl', sample_size=1000):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load a representative sample of the training data\n",
    "    check_sample = check.sample(min(sample_size, len(check)), random_state=42)\n",
    "\n",
    "    # Load or fit the vectorizer on the sample\n",
    "    try:\n",
    "        with open(vectorizer_path, 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(check_sample['cleaned_summary'])\n",
    "        with open(vectorizer_path, 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "\n",
    "    # Check vocabulary size\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "    # Apply the same text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Check the vocabulary size of the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "    print(f\"New data vocabulary size: {new_tokens.shape[1]}\")\n",
    "\n",
    "    # Use the vectorizer fitted on the sample\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    for summary, predicted_label in zip([new_summary], predicted_labels):\n",
    "        print(f\"Summary: {summary} | Predicted Genre: {predicted_label}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# preprocess_and_predict_2(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download the WordNet lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Assuming you have label_encoder defined\n",
    "label_encoder = ...\n",
    "\n",
    "# Assuming check is your DataFrame\n",
    "check = ...\n",
    "\n",
    "def preprocess_and_predict_3(new_summary, model_path='xgboost.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load or fit the vectorizer on the new summary\n",
    "    try:\n",
    "        with open(vectorizer_path, 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit([new_summary])  # Use the new data as the sample\n",
    "        with open(vectorizer_path, 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "\n",
    "    # Check vocabulary size\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "    # Apply the same text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Check the vocabulary size of the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "    print(f\"New data vocabulary size: {new_tokens.shape[1]}\")\n",
    "\n",
    "    # Use the vectorizer fitted on the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    print(f\"Summary: {new_summary} | Predicted Genre: {predicted_labels[0]}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# preprocess_and_predict_3(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Your text preprocessing steps here\n",
    "    # ...\n",
    "\n",
    "def preprocess_and_predict_4(new_summary, model_path='xgboost.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load or fit the vectorizer on the new summary\n",
    "    try:\n",
    "        with open(vectorizer_path, 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit([new_summary])  # Use the new data as the sample\n",
    "        with open(vectorizer_path, 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "\n",
    "    # Check vocabulary size\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "    # Apply the same text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Check the vocabulary size of the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "    print(f\"New data vocabulary size: {new_tokens.shape[1]}\")\n",
    "\n",
    "    # Use the vectorizer fitted on the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Recreate label encoder based on predicted labels\n",
    "    recreated_label_encoder = LabelEncoder()\n",
    "    recreated_label_encoder.fit(check['genre'])  # Use the genre column from your training data\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = recreated_label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    print(f\"Summary: {new_summary} | Predicted Genre: {predicted_labels[0]}\")\n",
    "\n",
    "# Example usage\n",
    "new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "preprocess_and_predict_4(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def preprocess_and_predict_5(new_summary, model_path='xgboost.pkl', vectorizer=None):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Use the provided vectorizer or create a new one\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the training data\n",
    "    check['preprocessed_summary'] = check['cleaned_summary'].apply(preprocess_text)\n",
    "\n",
    "    # Apply the same text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Check vocabulary size\n",
    "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "    # Load or fit the vectorizer on the new summary\n",
    "    vectorizer.fit(check['preprocessed_summary'])  # Use the preprocessed data as the sample\n",
    "\n",
    "    # Check the vocabulary size of the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "    print(f\"New data vocabulary size: {new_tokens.shape[1]}\")\n",
    "\n",
    "    # Use the vectorizer fitted on the new data\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genres for the new data\n",
    "    print(f\"Summary: {new_summary} | Predicted Genre: {predicted_labels[0]}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# preprocess_and_predict(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The Left Hand of Darkness by Ursula K. Le Guin (1969)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m new_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe novel follows the story of Genly Ai, a human native of Terra, who is sent to the planet of Gethen as an envoy of the Ekumen, a loose confederation of planets. Ai\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms mission is to persuade the nations of Gethen to join the Ekumen, but he is stymied by a lack of understanding of their culture. Individuals on Gethen are ambisexual, with no fixed sex; this has a strong influence on the culture of the planet, and creates a barrier of understanding for Ai.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m preprocess_and_predict_5(new_summary)\n",
      "Cell \u001b[0;32mIn[45], line 37\u001b[0m, in \u001b[0;36mpreprocess_and_predict_5\u001b[0;34m(new_summary, model_path, vectorizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Apply text preprocessing steps to the training data\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m check[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_summary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply the same text preprocessing steps to the new summary\u001b[39;00m\n\u001b[1;32m     40\u001b[0m new_preprocessed_summary \u001b[38;5;241m=\u001b[39m preprocess_text(new_summary)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# The Left Hand of Darkness by Ursula K. Le Guin (1969)\n",
    "\n",
    "new_summary = \"The novel follows the story of Genly Ai, a human native of Terra, who is sent to the planet of Gethen as an envoy of the Ekumen, a loose confederation of planets. Ai's mission is to persuade the nations of Gethen to join the Ekumen, but he is stymied by a lack of understanding of their culture. Individuals on Gethen are ambisexual, with no fixed sex; this has a strong influence on the culture of the planet, and creates a barrier of understanding for Ai.\"\n",
    "preprocess_and_predict_5(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>genre</th>\n",
       "      <th>cleaned_summary</th>\n",
       "      <th>entities</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>alex teenager living near future england lead ...</td>\n",
       "      <td>[('alex', 'PERSON'), ('england', 'GPE'), ('rus...</td>\n",
       "      <td>588</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>text plague divided five part town oran thousa...</td>\n",
       "      <td>[('five', 'CARDINAL'), ('dr bernard rieux', 'P...</td>\n",
       "      <td>609</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Quiet on the Western Front</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "      <td>literary fiction</td>\n",
       "      <td>book tell story paul umer german soldier who u...</td>\n",
       "      <td>[('paul umer', 'PERSON'), ('german', 'NORP'), ...</td>\n",
       "      <td>375</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Wizard of Earthsea</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>ged young boy gont one larger island north arc...</td>\n",
       "      <td>[('gont', 'PERSON'), ('one', 'CARDINAL'), ('ar...</td>\n",
       "      <td>549</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blade Runner 3: Replicant Night</td>\n",
       "      <td>Living on Mars, Deckard is acting as a consul...</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>living mar deckard acting consultant movie cre...</td>\n",
       "      <td>[('mar deckard', 'PERSON')]</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0               A Clockwork Orange   \n",
       "1                       The Plague   \n",
       "2   All Quiet on the Western Front   \n",
       "3             A Wizard of Earthsea   \n",
       "4  Blade Runner 3: Replicant Night   \n",
       "\n",
       "                                             summary             genre  \\\n",
       "0   Alex, a teenager living in near-future Englan...   science fiction   \n",
       "1   The text of The Plague is divided into five p...  literary fiction   \n",
       "2   The book tells the story of Paul Bäumer, a Ge...  literary fiction   \n",
       "3   Ged is a young boy on Gont, one of the larger...           fantasy   \n",
       "4   Living on Mars, Deckard is acting as a consul...   science fiction   \n",
       "\n",
       "                                     cleaned_summary  \\\n",
       "0  alex teenager living near future england lead ...   \n",
       "1  text plague divided five part town oran thousa...   \n",
       "2  book tell story paul umer german soldier who u...   \n",
       "3  ged young boy gont one larger island north arc...   \n",
       "4  living mar deckard acting consultant movie cre...   \n",
       "\n",
       "                                            entities  word_count  \\\n",
       "0  [('alex', 'PERSON'), ('england', 'GPE'), ('rus...         588   \n",
       "1  [('five', 'CARDINAL'), ('dr bernard rieux', 'P...         609   \n",
       "2  [('paul umer', 'PERSON'), ('german', 'NORP'), ...         375   \n",
       "3  [('gont', 'PERSON'), ('one', 'CARDINAL'), ('ar...         549   \n",
       "4                        [('mar deckard', 'PERSON')]          27   \n",
       "\n",
       "   unique_word_count  \n",
       "0                416  \n",
       "1                424  \n",
       "2                277  \n",
       "3                371  \n",
       "4                 27  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predict_genre(new_summary, model_path='xgboost_2.pkl', label_encoder_path='label_encoder.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    # Load the pre-trained model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "\n",
    "    # Load the label encoder\n",
    "    with open(label_encoder_path, 'rb') as file:\n",
    "        label_encoder = pickle.load(file)\n",
    "\n",
    "    # Load the vectorizer\n",
    "    with open(vectorizer_path, 'rb') as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Vectorize the new summary using the loaded vectorizer\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = loaded_model.predict(new_tokens)\n",
    "\n",
    "    # Decode numerical predictions back to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "    # Display the predicted genre for the new data\n",
    "    print(f\"Predicted Genre: {predicted_labels[0]}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# predict_genre(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def predict_genre(new_summary, model_path='xgboost_2.pkl', label_encoder_path='label_encoder.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    try:\n",
    "        # Load the pre-trained model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the label encoder\n",
    "        label_encoder = joblib.load(label_encoder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the label encoder: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the vectorizer\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the vectorizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Vectorize the new summary using the loaded vectorizer\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    try:\n",
    "        predictions = loaded_model.predict(new_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assume label_encoder is a list of labels\n",
    "    try:\n",
    "        predicted_label = label_encoder[predictions[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # Display the predicted genre for the new data\n",
    "    print(f\"Predicted Genre: {predicted_label}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# predict_genre(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre: 2\n"
     ]
    }
   ],
   "source": [
    "# The Left Hand of Darkness by Ursula K. Le Guin (1969)\n",
    "\n",
    "new_summary = \"Nine noble families fight for control over the lands of Westeros, while an ancient enemy returns after being dormant for a millennia\"\n",
    "predict_genre(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def predict_genre(new_summary, model_path='xgboost_2.pkl', label_encoder_path='label_encoder.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    try:\n",
    "        # Load the pre-trained model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the label encoder\n",
    "        label_encoder = joblib.load(label_encoder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the label encoder: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the vectorizer\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the vectorizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Vectorize the new summary using the loaded vectorizer\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    try:\n",
    "        predictions = loaded_model.predict(new_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # Display the predicted genre for the new data\n",
    "    print(f\"Predicted Genre (Numerical Label): {predictions[0]}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# predict_genre(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre (Numerical Label): 1\n"
     ]
    }
   ],
   "source": [
    "# The Left Hand of Darkness by Ursula K. Le Guin (1969)\n",
    "\n",
    "new_summary = \"The novel follows the story of Genly Ai, a human native of Terra, who is sent to the planet of Gethen as an envoy of the Ekumen, a loose confederation of planets. Ai's mission is to persuade the nations of Gethen to join the Ekumen, but he is stymied by a lack of understanding of their culture. Individuals on Gethen are ambisexual, with no fixed sex; this has a strong influence on the culture of the planet, and creates a barrier of understanding for Ai\"\n",
    "predict_genre(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def predict_genre(new_summary, model_path='xgboost_2.pkl', label_encoder_path='label_encoder.pkl', vectorizer_path='tfidf_vectorizer.pkl'):\n",
    "    try:\n",
    "        # Load the pre-trained model\n",
    "        loaded_model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the label encoder\n",
    "        label_encoder = joblib.load(label_encoder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the label encoder: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the vectorizer\n",
    "        vectorizer = joblib.load(vectorizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the vectorizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define a mapping between numerical labels and genre names\n",
    "    label_mapping = {\n",
    "        0: 'Fantasy',\n",
    "        1: 'Historical Novel',\n",
    "        2: 'Literary Fiction',\n",
    "        3: 'Science Fiction',\n",
    "        4: 'Thriller'\n",
    "       \n",
    "    }\n",
    "\n",
    "    # Tokenization, lowercasing, stop word removal, lemmatization, and special character/number removal\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Apply text preprocessing steps to the new summary\n",
    "    new_preprocessed_summary = preprocess_text(new_summary)\n",
    "\n",
    "    # Vectorize the new summary using the loaded vectorizer\n",
    "    new_tokens = vectorizer.transform([new_preprocessed_summary])\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    try:\n",
    "        predictions = loaded_model.predict(new_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # Map the numerical label to the corresponding genre name\n",
    "    predicted_genre = label_mapping.get(predictions[0], 'Unknown Genre')\n",
    "\n",
    "    # Display the predicted genre for the new data\n",
    "    print(f\"Predicted Genre: {predicted_genre}\")\n",
    "\n",
    "# Example usage\n",
    "# new_summary = \"The novel follows the story of Genly Ai, a human native of Terra...\"\n",
    "# predict_genre(new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre: Historical Novel\n"
     ]
    }
   ],
   "source": [
    "new_summary = '''\n",
    "\n",
    "Though he battled for years to marry her, Henry VIII has become disenchanted with the audacious Anne Boleyn. She has failed to give him a son, and her sharp intelligence and strong will have alienated his old friends and the noble families of England.\n",
    "\n",
    "When the discarded Katherine, Henry's first wife, dies in exile from the court, Anne stands starkly exposed, the focus of gossip and malice, setting in motion a dramatic trial of the queen and her suitors for adultery and treason.\n",
    "\n",
    "At a word from Henry, Thomas Cromwell is ready to bring her down. Over a few terrifying weeks, Anne is ensnared in a web of conspiracy, while the demure Jane Seymour stands waiting her turn for the poisoned wedding ring. But Anne and her powerful family will not yield without a ferocious struggle. To defeat the Boleyns, Cromwell must ally himself with his enemies. What price will he pay for Annie's head?\"\n",
    "'''\n",
    "\n",
    "predict_genre(new_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
